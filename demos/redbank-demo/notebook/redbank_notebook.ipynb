{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the llama stack client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama_stack==0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://redbank-lsd-service:8321\")\n",
    "client.models.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set LLM and Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://redbank-lsd-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "model_id = \"vllm-inference/qwen2-5\"\n",
    "embedding_model_id = (\n",
    "    em := next(m for m in models if m.model_type == \"embedding\")\n",
    ").identifier\n",
    "embedding_dimension = em.metadata[\"embedding_dimension\"]\n",
    "\n",
    "print(model_id)\n",
    "print(embedding_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the LLM with RAG and MCP tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(prompt: str):\n",
    "    from llama_stack_client import LlamaStackClient\n",
    "\n",
    "    client = LlamaStackClient(base_url=\"http://redbank-lsd-service:8321\")\n",
    "\n",
    "    vector_stores = client.vector_stores.list()\n",
    "    vector_store = next(\n",
    "        (s for s in vector_stores.data if s.name == \"redbank-kb-vector-store\"), None\n",
    "    )\n",
    "    print(f\"Vector store ID: {vector_store.id}\")\n",
    "\n",
    "    resp = client.responses.create(\n",
    "        model=model_id,\n",
    "        instructions=\"\"\"\n",
    "            You are a helpful assistant with access to financial data through MCP tools.\n",
    "\n",
    "            IMPORTANT: Transaction all data is from 2025. Use dates like '2025-01-01'.\n",
    "\n",
    "            When asked questions, use available tools to find the answer. Follow these rules:\n",
    "\n",
    "            1. Use tools immediately without asking for confirmation\n",
    "            2. If you need additional information, search for it using whatever details are provided\n",
    "            3. Chain tool calls as needed - use results from one call as inputs to the next\n",
    "            4. If one approach doesn't work, try alternative methods silently\n",
    "            5. Do not narrate your process, explain failures, or describe what you're trying - just do it\n",
    "            6. Only provide output when you have the final answer\n",
    "            7. If you truly cannot find the information after multiple attempts, simply state what you were unable to find\n",
    "            8. Use the rag tool to answer all questions related to knowledge base or FAQs when the question is not about transactions or user-specific data.\n",
    "\n",
    "            Just execute tool calls until you have an answer, then provide it.\n",
    "        \"\"\",\n",
    "        tools=[\n",
    "            {\n",
    "                \"type\": \"mcp\",\n",
    "                \"server_label\": \"dmcp\",\n",
    "                \"server_description\": \"MCP Server.\",\n",
    "                \"server_url\": \"http://redbank-mcp-server:8000/mcp\",\n",
    "                \"require_approval\": \"never\",\n",
    "            },\n",
    "            {\"type\": \"file_search\", \"vector_store_ids\": [vector_store.id]},\n",
    "        ],\n",
    "        input=prompt,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    full_text = \"\"\n",
    "    for event in resp:\n",
    "        if event.type == \"response.output_text.delta\":\n",
    "            print(event.delta, end=\"\", flush=True)\n",
    "            full_text += event.delta\n",
    "        elif event.type == \"response.completed\":\n",
    "            print(\"\\n\\n--- Stream complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query that uses MCP tools\n",
    "run_query(\"How much was David's client lunch, and what is his email?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query that uses RAG tools\n",
    "run_query(\"Who founded Red Bank Financial?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test your Speech-To-Text model (Whisper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_transcribe(audio_file_path: str):\n",
    "    from openai import OpenAI\n",
    "\n",
    "    WHISPER_URL = \"http://whisper-large-v3-turbo-quantized-predictor:80/v1\"\n",
    "    AUDIO_FILE = audio_file_path\n",
    "\n",
    "    client = OpenAI(base_url=WHISPER_URL, api_key=\"fake\")\n",
    "\n",
    "    with open(AUDIO_FILE, \"rb\") as audio_file:\n",
    "        transcript = client.audio.transcriptions.create(\n",
    "            model=\"whisper-large-v3-turbo-quantized\", file=audio_file, language=\"en\"\n",
    "        )\n",
    "    print(f\"üìù Transcription: {transcript.text}\")\n",
    "    return transcript.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_transcribe(\"who-founded-red-bank-financial.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test your Text-To-Speech model (Kokoro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kokoro_speak(input_text: str):\n",
    "    import openai\n",
    "    import httpx\n",
    "\n",
    "    unverified_client = httpx.Client(verify=False, timeout=120)\n",
    "    client = openai.OpenAI(\n",
    "        base_url=\"http://kokoro-fastapi.<namespace>.svc.cluster.local:8880/v1\",\n",
    "        api_key=\"not-needed\",\n",
    "        http_client=unverified_client,\n",
    "    )\n",
    "\n",
    "    # Stream Kokoro audio output directly to a WAV file\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"kokoro\",\n",
    "        voice=\"af_bella\",\n",
    "        input=input_text,\n",
    "        format=\"wav\",\n",
    "    ) as response:\n",
    "        response.stream_to_file(\"response.wav\")\n",
    "\n",
    "    from IPython.display import Audio, display\n",
    "\n",
    "    display(Audio(\"response.wav\"))\n",
    "\n",
    "    print(\"‚úÖ Audio saved to response.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kokoro_speak(\"Red Bank Financial was founded by Red Hatters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
