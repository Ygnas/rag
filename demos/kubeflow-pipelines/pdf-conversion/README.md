# ðŸš€ PDF Ingestion and RAG Indexing with LlamaStack, Milvus, and Data Science Pipelines on OpenShift AI

## Overview

This pipeline converts your PDF documents from a GitHub source to text chunks, generates embeddings with a SentenceTransformer model, and inserts those embeddings into your vector store via Llama Stack for efficient vector search. Itâ€™s designed to run on Kubeflow Pipelines with GPU acceleration support.

## Prerequisites

- Red Hat OpenShift AI v3.0+
- Data science project created with a configured pipeline server and workbench with Python 3.12.
- LlamaStack Operator enabled in the DSC resource. See [Working with Llama Stack](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/working_with_llama_stack/index).
- LlamaStackDistribution deployed and configured with the `qwen3-14b-awq` instruct model:
    - See [common-deployments](../../common-deployments). Apply the Qwen3 model first, and once it's ready, apply the llamastackdistribution resource.
    - Alternatively, you can use your own instruct model.
- 1â€“2 NVIDIA GPUs (one for the instruct model, and optionally one for the pipeline run)

## Import and run the KubeFlow Pipeline

Import the "[pdf_rag_pipeline_compiled.yaml](pdf_rag_pipeline_compiled.yaml)" KubeFlow Pipeline into your pipeline server, then run the pipeline to insert your PDF documents into the vector store.

When running the pipeline, you can customize the following parameters:

- `base_url`: The base web URL where the source PDF files are located.
- `pdf_filenames`: Comma-separated list of PDF filenames to download from the base_url
- `vector_store_name`: Milvus vector store name
- `service_url`: Milvus service URL
- `embedding_model_id`: Embedding model to use
- `max_tokens`: Maximum tokens per chunk
- `chunk_overlap_tokens`: Chunk overlap size in tokens
- `use_gpu`: Enable/disable GPU acceleration

> Note: The compiled pipeline was generated by running `python pdf_rag_pipeline.py`.

## Prompt the LLM

Once your documents are embedded and indexed, you can query them by running through the example notebook [pdf_llama_stack_rag.ipynb](pdf_llama_stack_rag.ipynb)
