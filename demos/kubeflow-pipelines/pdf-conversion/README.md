# ðŸš€ PDF Ingestion and RAG Indexing with LlamaStack, Milvus, and Data Science Pipelines on OpenShift AI

## Overview

This pipeline converts your PDF documents from a GitHub source to text chunks, generates embeddings with a SentenceTransformer model, and inserts those embeddings into your vector store via Llama Stack for efficient vector search. Itâ€™s designed to run on Kubeflow Pipelines with GPU acceleration support.

## Prerequisites

- Red Hat OpenShift AI v3.0
- Data science project created with a configured pipeline server and workbench with Python 3.12.
- [LlamaStack Operator](https://github.com/opendatahub-io/llama-stack-k8s-operator) enabled in the DSC resource
- LlamaStackDistribution custom resource [configured](../../../stack/README.md) with the llama-3.2:3b instruct model. Alternatively, you can use your own instruct model.
- 1â€“2 NVIDIA GPUs (one for the instruct model, and optionally one for the pipeline run)

## Import and run the KubeFlow Pipeline

Import the "[pdf_rag_pipeline_compiled.yaml](pdf_rag_pipeline_compiled.yaml)" KubeFlow Pipeline into your pipeline server, then run the pipeline to insert your PDF documents into the vector store.

When running the pipeline, you can customize the following parameters:

- `base_url`: The base web URL where the source PDF files are located.
- `pdf_filenames`: Comma-separated list of PDF filenames to download from the base_url
- `vector_store_name`: Milvus vector store name
- `service_url`: Milvus service URL
- `embedding_model_id`: Embedding model to use
- `max_tokens`: Maximum tokens per chunk
- `chunk_overlap_tokens`: Chunk overlap size in tokens
- `use_gpu`: Enable/disable GPU acceleration

> Note: The compiled pipeline was generated by running `python pdf_rag_pipeline.py`.

## Prompt the LLM

Once your documents are embedded and indexed, you can query them by running through the example notebook [llama_stack_rag.ipynb](llama_stack_rag.ipynb)
