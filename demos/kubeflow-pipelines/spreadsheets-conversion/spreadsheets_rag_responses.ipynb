{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "## 1. Create Llama Stack client, list available models and vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780889b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://lsd-milvus-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(f\"Models information: {models}\\n\")\n",
    "\n",
    "inference_llm = next(\n",
    "    m\n",
    "    for m in models\n",
    "    if m.custom_metadata and m.custom_metadata.get(\"model_type\") == \"llm\"\n",
    ")\n",
    "print(f\"Identifier for Inference model in usage: {inference_llm}\\n\")\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Stores ===\")\n",
    "client.vector_stores.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## 2. Prompt the LLM and retrieve relevant context via RAG\n",
    "Prompt the LLM with questions in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LlamaStackClient(base_url=\"http://lsd-milvus-service:8321\")\n",
    "\n",
    "vector_stores = client.vector_stores.list()\n",
    "vector_store = next(\n",
    "    (s for s in vector_stores.data if s.name == \"csv-vector-store\"), None\n",
    ")\n",
    "\n",
    "user_prompts = [\n",
    "    \"What is gender, home country and age of Dulce Abril and Philip Gent?\",\n",
    "    \"What is customer id, company, city, country, phone number, email, subscription date, subscribed website of of Sheryl Baxter?\",\n",
    "    \"What products were sold according to sample sales data?\",\n",
    "    \"What is the economics condition at Ireland in 2025?\",  # Dummy question the model will answer with 'I don’t know'\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    resp = client.responses.create(\n",
    "        model=inference_llm.id,\n",
    "        instructions=\"\"\"\n",
    "            /no_think\n",
    "            You are a helpful assistant with access to data via the file_search tool.\n",
    "\n",
    "            When asked questions, use available tools to find the answer. Follow these rules:\n",
    "            1. Use tools immediately without asking for confirmation\n",
    "            2. Chain tool calls as needed\n",
    "            3. Do not narrate your process\n",
    "            4. Only provide the final answer\n",
    "            5. If the answer is not found in the context, respond with 'I don’t know'\n",
    "        \"\"\",\n",
    "        tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vector_store.id]}],\n",
    "        stream=False,\n",
    "        input=prompt,\n",
    "    )\n",
    "    responses.append(resp)\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {resp.output_text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "source": [
    "## 3. Preparation for evaluating RAG models using [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/?h=metrics)\n",
    "\n",
    "- We will use two key metrics to show the performance of the RAG server:\n",
    "    1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) - measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "    2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.\n",
    "\n",
    " - Create and paste your API key from [Groq Cloud](https://console.groq.com/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "# Extract retrieved contexts from Responses API output\n",
    "def extract_retrieved_contexts(response) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts retrieved contexts from LlamaStack Responses API output.\n",
    "\n",
    "    Args:\n",
    "        response: Response object from client.responses.create()\n",
    "\n",
    "    Returns:\n",
    "        List of retrieved context strings for Ragas evaluation\n",
    "    \"\"\"\n",
    "    retrieved_contexts = []\n",
    "\n",
    "    for output_item in response.output:\n",
    "        # Check if this is a file_search_call with results\n",
    "        if (\n",
    "            hasattr(output_item, \"type\")\n",
    "            and output_item.type == \"file_search_call\"\n",
    "            and hasattr(output_item, \"results\")\n",
    "            and output_item.results\n",
    "        ):\n",
    "            for result in output_item.results:\n",
    "                if hasattr(result, \"text\") and result.text:\n",
    "                    retrieved_contexts.append(result.text)\n",
    "\n",
    "    return retrieved_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5043ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "references = [\n",
    "    \"\"\"\n",
    "Dulce Abril is 32 years old female from USA and Philip Gent is 36 years old man from France.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Sheryl Baxter's customer ID is DD37Cf93aecA6Dc, her company is Rasmussen Group, her city is East Leonard, her country is Chile, her phone numbers are 229.077.5154 and 397.884.0519x718, her email is zunigavanessa@smith.info, her subscription date is 2020-08-24, and her subscribed website is http://www.stephenson.com/.\n",
    "\"\"\",\n",
    "]\n",
    "\n",
    "# Constructing a Ragas EvaluationDataset\n",
    "for i, response in enumerate(responses[: len(references)]):\n",
    "    samples.append(\n",
    "        {\n",
    "            \"user_input\": user_prompts[i],\n",
    "            \"response\": response.output_text,\n",
    "            \"reference\": references[i],\n",
    "            \"retrieved_contexts\": extract_retrieved_contexts(response),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset.from_list(samples)\n",
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## 4. Prerequisites for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    ")\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Wrap the Groq LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Using HuggingFace embeddings as a free alternative\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ibm-granite/granite-embedding-125m-english\"\n",
    ")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_model)\n",
    "\n",
    "\n",
    "# references for both prompts\n",
    "reference_for_first_prompt = samples[0][\"reference\"]\n",
    "reference_for_second_prompt = samples[1][\"reference\"]\n",
    "\n",
    "# inputs for both prompts\n",
    "user_input_for_first_prompt = samples[0][\"user_input\"]\n",
    "user_input_for_second_prompt = samples[1][\"user_input\"]\n",
    "\n",
    "# responses for both prompts\n",
    "response_for_first_prompt = samples[0][\"response\"]\n",
    "response_for_second_prompt = samples[1][\"response\"]\n",
    "\n",
    "# reference lists for both prompts\n",
    "reference_list_for_first_prompt = [\n",
    "    line.strip() for line in reference_for_first_prompt.strip().split(\"\\n\")\n",
    "]\n",
    "reference_list_for_second_prompt = [\n",
    "    line.strip() for line in reference_for_second_prompt.strip().split(\"\\n\")\n",
    "]\n",
    "\n",
    "# Retrieved contexts for both prompts\n",
    "retrieved_contexts_for_first_prompt = samples[0][\"retrieved_contexts\"]\n",
    "retrieved_contexts_for_second_prompt = samples[1][\"retrieved_contexts\"]\n",
    "\n",
    "print(\n",
    "    f\"Retrieved contexts for the first prompt: {retrieved_contexts_for_first_prompt}\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"Retrieved contexts for the second prompt: {retrieved_contexts_for_second_prompt}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "## 5. Evaluate Faithfulness Score for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_first_prompt,\n",
    "    response=response_for_first_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    ")\n",
    "faithfulness_scorer = Faithfulness(llm=evaluator_llm)\n",
    "faithfulness_score_for_first_prompt = await faithfulness_scorer.single_turn_ascore(\n",
    "    first_prompt_turn\n",
    ")\n",
    "\n",
    "second_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_second_prompt,\n",
    "    response=response_for_second_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    ")\n",
    "faithfulness_score_for_second_prompt = await faithfulness_scorer.single_turn_ascore(\n",
    "    second_prompt_turn\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Faithfulness score for prompt '{user_prompts[0]}': {faithfulness_score_for_first_prompt}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Faithfulness score for prompt '{user_prompts[1]}': {faithfulness_score_for_second_prompt}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## 6. Evaluate Response Relevancy for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_first_prompt,\n",
    "    response=response_for_first_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    ")\n",
    "response_relevancy_scorer = ResponseRelevancy(\n",
    "    llm=evaluator_llm, embeddings=evaluator_embeddings\n",
    ")\n",
    "response_relevancy_score_for_first_prompt = (\n",
    "    await response_relevancy_scorer.single_turn_ascore(first_prompt_turn)\n",
    ")\n",
    "\n",
    "second_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_second_prompt,\n",
    "    response=response_for_second_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    ")\n",
    "response_relevancy_score_for_second_prompt = (\n",
    "    await response_relevancy_scorer.single_turn_ascore(second_prompt_turn)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Response Relevancy score for prompt '{user_prompts[0]}': {response_relevancy_score_for_first_prompt}\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Response Relevancy score for prompt '{user_prompts[1]}': {response_relevancy_score_for_second_prompt}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
