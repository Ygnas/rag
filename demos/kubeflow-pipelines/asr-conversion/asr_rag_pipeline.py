# Copyright 2025 IBM, Red Hat
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ruff: noqa: PLC0415,UP007,UP035,UP006,E712
# SPDX-License-Identifier: Apache-2.0
from kfp import compiler, dsl
from kfp.kubernetes import add_node_selector_json, add_toleration_json

PYTHON_BASE_IMAGE = "registry.redhat.io/ubi9/python-312@sha256:e80ff3673c95b91f0dafdbe97afb261eab8244d7fd8b47e20ffcbcfee27fb168"

# Workbench Runtime Image: Pytorch with CUDA and Python 3.12 (UBI 9)
# The images for each release can be found in
# https://github.com/red-hat-data-services/rhoai-disconnected-install-helper/blob/main/rhoai-2.23.md
PYTORCH_CUDA_IMAGE = "quay.io/modh/odh-pipeline-runtime-pytorch-cuda-py312-ubi9@sha256:72ff2381e5cb24d6f549534cb74309ed30e92c1ca80214669adb78ad30c5ae12"


@dsl.component(
    base_image=PYTORCH_CUDA_IMAGE,
    packages_to_install=[
        "llama-stack-client==0.3.0",
        "fire",
        "requests",
        "openai-whisper",
    ],
)
def register_vector_store_and_files(
    service_url: str,
    vector_store_name: str,
    embedding_model_id: str,
    max_tokens: int,
    chunk_overlap_tokens: int,
    base_url: str,
    audio_filenames: str,
):
    import io
    import os
    import tempfile
    import requests
    import whisper
    from llama_stack_client import LlamaStackClient

    # ffmpeg is required for OpenAI Whisper to decode audio inputs
    def download_and_install_ffmpeg():
        import os
        import subprocess

        try:
            print("Downloading static ffmpeg binary...")

            import urllib.request
            import stat
            import pathlib
            import shutil

            # Create temp directory
            temp_dir = pathlib.Path("/tmp/ffmpeg_install")
            temp_dir.mkdir(exist_ok=True)

            # Download static ffmpeg binary using pinned build version.
            # Build: https://github.com/BtbN/FFmpeg-Builds/releases/tag/autobuild-2025-12-01-12-56
            build_id = "autobuild-2025-12-01-12-56"
            artifact = "ffmpeg-N-121951-g7043522fe0-linux64-gpl.tar.xz"
            ffmpeg_url = f"https://github.com/BtbN/FFmpeg-Builds/releases/download/{build_id}/{artifact}"
            ffmpeg_archive = temp_dir / "ffmpeg-static.tar.xz"

            print(f"Downloading ffmpeg from {ffmpeg_url}")
            urllib.request.urlretrieve(ffmpeg_url, ffmpeg_archive)

            print("Extracting ffmpeg archive...")
            subprocess.run(
                ["tar", "-xf", str(ffmpeg_archive), "-C", str(temp_dir)],
                check=True,
                capture_output=True,
            )

            ffmpeg_path_candidates = list(temp_dir.rglob("ffmpeg"))
            if not ffmpeg_path_candidates:
                raise FileNotFoundError(
                    "Could not find 'ffmpeg' executable in the extracted archive."
                )

            ffmpeg_path = ffmpeg_path_candidates[0]

            # Make executable
            ffmpeg_path.chmod(
                ffmpeg_path.stat().st_mode | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH
            )

            # Use a writable bin directory and add it to the PATH
            bin_dir = pathlib.Path("/tmp/bin")
            bin_dir.mkdir(exist_ok=True)
            target_path = bin_dir / "ffmpeg"

            # Move the file to the target path
            ffmpeg_path.rename(target_path)

            # Add to PATH environment variable
            os.environ["PATH"] = f"{str(bin_dir)}:{os.environ.get('PATH', '')}"

            # Verify installation
            subprocess.run(
                [str(target_path), "-version"], capture_output=True, check=True
            )
            print(f"Static ffmpeg binary installed to {target_path} and added to PATH")

            # Clean up extraction directory
            shutil.rmtree(temp_dir, ignore_errors=True)

        except Exception as e:
            print(f"Failed to install ffmpeg: {e}")
            raise RuntimeError(
                "ffmpeg installation failed. Audio processing requires ffmpeg."
            ) from e

    # Install ffmpeg for audio processing before main logic
    download_and_install_ffmpeg()

    # Load Whisper model
    print("Loading Whisper model...")
    whisper_model = whisper.load_model("base")

    client = LlamaStackClient(base_url=service_url)

    # Transcribe audio files and upload transcripts
    file_ids = []
    for filename in audio_filenames.split(","):
        source = f"{base_url}/{filename.strip()}"
        print(f"Downloading audio file: {source}")

        try:
            # Download the audio file
            response = requests.get(source)
            response.raise_for_status()

            # Save to temporary file
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_audio:
                tmp_audio.write(response.content)
                tmp_audio_path = tmp_audio.name

            try:
                # Transcribe with Whisper
                print(f"Transcribing {filename.strip()}...")
                result = whisper_model.transcribe(tmp_audio_path)
                transcript_text = result["text"]
                print(f"Transcription complete: {len(transcript_text)} characters")

                # Upload transcript as text file
                file_basename = filename.strip().rsplit(".", 1)[0] + ".txt"
                transcript_content = io.BytesIO(transcript_text.encode("utf-8"))

                file = client.files.create(
                    file=(file_basename, transcript_content, "text/plain"),
                    purpose="assistants",
                )
                file_ids.append(file.id)
                print(
                    f"Successfully uploaded transcript {file_basename} (file_id: {file.id})"
                )

            finally:
                # Clean up temp file
                os.unlink(tmp_audio_path)

        except Exception as e:
            print(f"ERROR: Failed to process {filename.strip()}: {str(e)}")
            raise

    print(
        f"Successfully processed and uploaded {len(file_ids)} transcripts: {file_ids}"
    )

    models = client.models.list()
    matching_model = next(
        (m for m in models if m.provider_resource_id == embedding_model_id), None
    )

    if not matching_model:
        raise ValueError(
            f"Model with ID '{embedding_model_id}' not found on LlamaStack server."
        )

    if matching_model.api_model_type != "embedding":
        raise ValueError(f"Model '{embedding_model_id}' is not an embedding model")

    embedding_dimension = matching_model.metadata["embedding_dimension"]

    # Warm up the embedding model
    client.embeddings.create(
        model=embedding_model_id,
        input="warmup",
    )

    # Create empty vector store first, before inserting files.
    # Purpose: Depending on the size and number of files, attempting to create the vector store
    # and add files in a single step may lead to timeouts.
    try:
        vector_store = client.vector_stores.create(
            name=vector_store_name,
            file_ids=[],
            chunking_strategy={
                "type": "static",
                "static": {
                    "max_chunk_size_tokens": max_tokens,
                    "chunk_overlap_tokens": chunk_overlap_tokens,
                },
            },
            extra_body={
                "embedding_model": embedding_model_id,
                "embedding_dimension": embedding_dimension,
                "provider_id": "milvus",
            },
        )
        print(
            f"Successfully created vector store '{vector_store_name}' with ID: {vector_store.id}"
        )
    except Exception as e:
        print(f"ERROR: Failed to create vector store '{vector_store_name}': {str(e)}")
        raise

    # Add files to vector store
    try:
        for file_id in file_ids:
            print(f"Adding file_id '{file_id}' to vector store '{vector_store_name}'")
            client.vector_stores.files.create(
                vector_store_id=vector_store.id,
                file_id=file_id,
            )
        vector_store = client.vector_stores.retrieve(vector_store.id)
        print(f"Vector store details: {vector_store}")
    except Exception as e:
        print(f"WARNING: Some files failed to be added to vector store: {str(e)}")


@dsl.pipeline()
def vector_store_files_pipeline(
    base_url: str = "https://raw.githubusercontent.com/opendatahub-io/rag/main/demos/testing-data/audio-speech",
    audio_filenames: str = "RAG_use_cases.wav, RAG_customers.wav, RAG_benefits.m4a, RAG_vs_Regular_LLM_Output.m4a",
    vector_store_name: str = "asr-vector-store",
    service_url: str = "http://lsd-milvus-service:8321",
    embedding_model_id: str = "ibm-granite/granite-embedding-125m-english",
    max_tokens: int = 512,
    chunk_overlap_tokens: int = 64,
    use_gpu: bool = False,  # use only if you have additional gpu worker
) -> None:
    """
    Converts audio recordings to text using WHISPER ASR and generates embeddings
    :param base_url: Base URL to fetch audio files
    :param audio_filenames: Comma-separated list of audio filenames to download and convert
    :param vector_store_name: Name of the vector store to store embeddings
    :param service_url: URL of the LlamaStack service
    :param embedding_model_id: Model ID for embedding generation
    :param max_tokens: Maximum number of tokens per chunk
    :param use_gpu: boolean to enable/disable gpu
    :return:
    """

    with dsl.If(use_gpu == True):
        register_task = register_vector_store_and_files(
            service_url=service_url,
            vector_store_name=vector_store_name,
            embedding_model_id=embedding_model_id,
            max_tokens=max_tokens,
            chunk_overlap_tokens=chunk_overlap_tokens,
            base_url=base_url,
            audio_filenames=audio_filenames,
        )
        register_task.set_caching_options(False)
        register_task.set_cpu_request("500m")
        register_task.set_cpu_limit("4")
        register_task.set_memory_request("2Gi")
        register_task.set_memory_limit("6Gi")
        register_task.set_accelerator_type("nvidia.com/gpu")
        register_task.set_accelerator_limit(1)
        add_toleration_json(
            register_task,
            [
                {
                    "effect": "NoSchedule",
                    "key": "nvidia.com/gpu",
                    "operator": "Exists",
                }
            ],
        )
        add_node_selector_json(register_task, {})

    with dsl.Else():
        register_task = register_vector_store_and_files(
            service_url=service_url,
            vector_store_name=vector_store_name,
            embedding_model_id=embedding_model_id,
            max_tokens=max_tokens,
            chunk_overlap_tokens=chunk_overlap_tokens,
            base_url=base_url,
            audio_filenames=audio_filenames,
        )
        register_task.set_caching_options(False)
        register_task.set_cpu_request("500m")
        register_task.set_cpu_limit("4")
        register_task.set_memory_request("2Gi")
        register_task.set_memory_limit("6Gi")


if __name__ == "__main__":
    compiler.Compiler().compile(
        pipeline_func=vector_store_files_pipeline,
        package_path=__file__.replace(".py", "_compiled.yaml"),
    )
