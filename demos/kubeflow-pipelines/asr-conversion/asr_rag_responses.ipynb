{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Llama Stack client, list available models and vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "client = LlamaStackClient(base_url=\"http://lsd-milvus-service:8321\")\n",
    "\n",
    "models = client.models.list()\n",
    "print(f\"Models information: {models}\\n\")\n",
    "\n",
    "inference_llm = next(\n",
    "    (model.identifier for model in models if model.model_type == \"llm\"), None\n",
    ")\n",
    "print(f\"Identifier for Inference model in usage: {inference_llm}\\n\")\n",
    "\n",
    "# Check what vector databases exist\n",
    "print(\"=== Available Vector Stores ===\")\n",
    "client.vector_stores.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt the LLM and retrieve relevant context via RAG\n",
    "Prompt the LLM with questions in relation to the documents inserted, and see it return accurate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LlamaStackClient(base_url=\"http://lsd-milvus-service:8321\")\n",
    "\n",
    "vector_stores = client.vector_stores.list()\n",
    "vector_store = next(\n",
    "    (s for s in vector_stores.data if s.name == \"asr-vector-store\"), None\n",
    ")\n",
    "\n",
    "user_prompts = [\n",
    "    \"List RAG key market use cases\",\n",
    "    \"Name Red Hat RAG target audience and customers\",\n",
    "    \"What beneficial goals RAG support?\",\n",
    "    \"Regular LLM output disadvantages\",\n",
    "    \"What is the economics condition at Ireland in 2025?\",  # Dummy question the model will answer with 'I donâ€™t know' or reason why can't answer\n",
    "]\n",
    "\n",
    "responses = []\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    resp = client.responses.create(\n",
    "        model=inference_llm,\n",
    "        instructions=\"\"\"\n",
    "            You are a helpful assistant with access to data via the file_search tool.\n",
    "\n",
    "            When asked questions, use available tools to find the answer. Follow these rules:\n",
    "            1. Use tools immediately without asking for confirmation\n",
    "            2. Chain tool calls as needed\n",
    "            3. Do not narrate your process\n",
    "            4. Only provide the final answer\n",
    "        \"\"\",\n",
    "        tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vector_store.id]}],\n",
    "        stream=False,\n",
    "        input=prompt,\n",
    "    )\n",
    "    responses.append(resp)\n",
    "    print(f\"\\nQ: {prompt}\")\n",
    "    print(f\"A: {resp.output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparation for evaluating RAG models using [RAGAS](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/?h=metrics)\n",
    "\n",
    "- We will use two key metrics to show the performance of the RAG server:\n",
    "    1. [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) - measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "    2. [Response Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information.\n",
    "\n",
    " - Create and paste your API key from [Groq Cloud](https://console.groq.com/home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "# Extract retrieved contexts from Responses API output\n",
    "def extract_retrieved_contexts(response) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts retrieved contexts from LlamaStack Responses API output.\n",
    "\n",
    "    Args:\n",
    "        response: Response object from client.responses.create()\n",
    "\n",
    "    Returns:\n",
    "        List of retrieved context strings for Ragas evaluation\n",
    "    \"\"\"\n",
    "    retrieved_contexts = []\n",
    "\n",
    "    for output_item in response.output:\n",
    "        # Check if this is a file_search_call with results\n",
    "        if (\n",
    "            hasattr(output_item, \"type\")\n",
    "            and output_item.type == \"file_search_call\"\n",
    "            and hasattr(output_item, \"results\")\n",
    "            and output_item.results\n",
    "        ):\n",
    "            for result in output_item.results:\n",
    "                if hasattr(result, \"text\") and result.text:\n",
    "                    retrieved_contexts.append(result.text)\n",
    "\n",
    "    return retrieved_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import EvaluationDataset\n",
    "\n",
    "samples = []\n",
    "\n",
    "references = [\n",
    "    \"\"\"\n",
    "Key Market Use Cases\n",
    "RAG is being adopted across various industries for diverse applications, including:\n",
    "\n",
    "Knowledge Question Answering: Providing accurate answers in customer service using product manuals or FAQs.\n",
    "\n",
    "Code Generation: Retrieving relevant code snippets and documentation to assist in code creation.\n",
    "\n",
    "Recommendation Systems: Enhancing recommendations by providing relevant context.\n",
    "\n",
    "Customer Service: Improving support accuracy with access to current product information.\n",
    "\n",
    "Personal Assistants: Enabling more comprehensive and accurate information from AI assistants.\n",
    "\n",
    "Multi-hop Question Answering: Handling complex, multi-step questions through iterative retrieval.\n",
    "\n",
    "Legal Applications: Retrieving legal documents and case law for reliable legal opinions.\n",
    "\n",
    "General Task Assistance: Aiding users in various tasks requiring information access and decision-making.\n",
    "\n",
    "The rising demand for hyper-personalized content in areas like marketing and e-commerce is also a significant driver for RAG adoption, allowing for tailored ad copy and product recommendations.\n",
    "\"\"\",\n",
    "    \"\"\"\n",
    "Clarifying Target Audience and User Roles\n",
    "This document clarifies the target audience and user roles for our project, focusing on the distinction between end-users and builders.\n",
    "End Users vs. Builders:\n",
    "\n",
    "End Users: Consume the final product (e.g., interact with a ChatGPT-like application).\n",
    "Builders: Create and configure the AI systems used by end-users (e.g., configure a RAG backend, tweaking parameters for a specific experience such as ChatGPT).  We are targeting builders, not end-users. Builders optimize their systems for their specific end-users.\n",
    "\n",
    "Builder Archetypes:\n",
    "\n",
    "High-Coder Builders (aka pro-code): Prefer SDKs and code-based solutions. They need access to all configurable parameters via APIs and SDKs.  They may also want a quick way to \"vibe check\" their RAG system via a UI (e.g., llama-stack-cli my-rag-app.py --web).\n",
    "\n",
    "Low-Coder Builders (no/low-code): Prefer UI-driven workflows and visual tools to configure their systems.  They could benefit from tools like the existing llama-stack playground.\n",
    "\n",
    "Builders vs. Platformers vs. Opsers:\n",
    "\n",
    "Builders (AI Engineers/AI Devs): Use the platform and its primitives to build AI systems.  Their skillset and the complexity of their tasks determine whether they are considered AI Engineers or AI Devs.\n",
    "\n",
    "Platformers (AI Platform Engineers): Platformers focus on building, maintaining, and securing the AI platform and APIs. They serve both Builders (for development) and Opsers (for deployment/operations), ensuring infrastructure is reliable, scalable, and supports self-service.\n",
    "\n",
    "Opsers (AI/MLOps Engineers): Opsers focus on operationalizing and automating the AI/ML  lifecycle. For example, they use platform APIs to deploy, monitor, and manage models, enabling Builders' models to reach and succeed in production. Opsers work closely with Platformers to ensure infrastructure meets operational needs.\n",
    "\n",
    "In summary:\n",
    "\n",
    "Platformers enable builders, and builders create systems for end-users.  Our focus is on empowering builders with the tools and flexibility they need to build the best experiences for their end-users.\"\"\",\n",
    "]\n",
    "\n",
    "# Constructing a Ragas EvaluationDataset\n",
    "for i, response in enumerate(responses[: len(references)]):\n",
    "    samples.append(\n",
    "        {\n",
    "            \"user_input\": user_prompts[i],\n",
    "            \"response\": response.output_text,\n",
    "            \"reference\": references[i],\n",
    "            \"retrieved_contexts\": extract_retrieved_contexts(response),\n",
    "        }\n",
    "    )\n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset.from_list(samples)\n",
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prerequisites for RAG evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    ")\n",
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from langchain_groq import ChatGroq\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "# Wrap the Groq LLM for use with Ragas\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Using HuggingFace embeddings as a free alternative\n",
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"ibm-granite/granite-embedding-125m-english\"\n",
    ")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embeddings_model)\n",
    "\n",
    "\n",
    "# references for both prompts\n",
    "reference_for_first_prompt = samples[0][\"reference\"]\n",
    "reference_for_second_prompt = samples[1][\"reference\"]\n",
    "\n",
    "# inputs for both prompts\n",
    "user_input_for_first_prompt = samples[0][\"user_input\"]\n",
    "user_input_for_second_prompt = samples[1][\"user_input\"]\n",
    "\n",
    "# responses for both prompts\n",
    "response_for_first_prompt = samples[0][\"response\"]\n",
    "response_for_second_prompt = samples[1][\"response\"]\n",
    "\n",
    "# reference lists for both prompts\n",
    "reference_list_for_first_prompt = [\n",
    "    line.strip() for line in reference_for_first_prompt.strip().split(\"\\n\")\n",
    "]\n",
    "reference_list_for_second_prompt = [\n",
    "    line.strip() for line in reference_for_second_prompt.strip().split(\"\\n\")\n",
    "]\n",
    "\n",
    "# Retrieved contexts for both prompts\n",
    "retrieved_contexts_for_first_prompt = samples[0][\"retrieved_contexts\"]\n",
    "retrieved_contexts_for_second_prompt = samples[1][\"retrieved_contexts\"]\n",
    "\n",
    "print(\n",
    "    f\"Retrieved contexts for the first prompt: {retrieved_contexts_for_first_prompt}\\n\"\n",
    ")\n",
    "print(\n",
    "    f\"Retrieved contexts for the second prompt: {retrieved_contexts_for_second_prompt}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Faithfulness Score for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_first_prompt,\n",
    "    response=response_for_first_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    ")\n",
    "faithfulness_scorer = Faithfulness(llm=evaluator_llm)\n",
    "faithfulness_score_for_first_prompt = await faithfulness_scorer.single_turn_ascore(\n",
    "    first_prompt_turn\n",
    ")\n",
    "print(\n",
    "    f\"Faithfulness score for prompt '{user_prompts[0]}': {faithfulness_score_for_first_prompt}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_second_prompt,\n",
    "    response=response_for_second_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    ")\n",
    "faithfulness_score_for_second_prompt = await faithfulness_scorer.single_turn_ascore(\n",
    "    second_prompt_turn\n",
    ")\n",
    "print(\n",
    "    f\"Faithfulness score for prompt '{user_prompts[1]}': {faithfulness_score_for_second_prompt}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Response Relevancy for both prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_first_prompt,\n",
    "    response=response_for_first_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_first_prompt,\n",
    ")\n",
    "response_relevancy_scorer = ResponseRelevancy(\n",
    "    llm=evaluator_llm, embeddings=evaluator_embeddings\n",
    ")\n",
    "response_relevancy_score_for_first_prompt = (\n",
    "    await response_relevancy_scorer.single_turn_ascore(first_prompt_turn)\n",
    ")\n",
    "print(\n",
    "    f\"Response Relevancy score for prompt '{user_prompts[0]}': {response_relevancy_score_for_first_prompt}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt_turn = SingleTurnSample(\n",
    "    user_input=user_input_for_second_prompt,\n",
    "    response=response_for_second_prompt,\n",
    "    retrieved_contexts=retrieved_contexts_for_second_prompt,\n",
    ")\n",
    "response_relevancy_score_for_second_prompt = (\n",
    "    await response_relevancy_scorer.single_turn_ascore(second_prompt_turn)\n",
    ")\n",
    "print(\n",
    "    f\"Response Relevancy score for prompt '{user_prompts[1]}': {response_relevancy_score_for_second_prompt}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
