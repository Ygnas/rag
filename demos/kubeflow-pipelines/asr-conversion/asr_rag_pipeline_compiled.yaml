# PIPELINE DEFINITION
# Name: vector-store-files-pipeline
# Description: Converts audio recordings to text using WHISPER ASR and generates embeddings
# Inputs:
#    audio_filenames: str [Default: 'RAG_use_cases.wav, RAG_customers.wav, RAG_benefits.m4a, RAG_vs_Regular_LLM_Output.m4a']
#    base_url: str [Default: 'https://raw.githubusercontent.com/opendatahub-io/rag/main/demos/testing-data/audio-speech']
#    chunk_overlap_tokens: int [Default: 64.0]
#    embedding_model_id: str [Default: 'ibm-granite/granite-embedding-125m-english']
#    max_tokens: int [Default: 512.0]
#    service_url: str [Default: 'http://lsd-milvus-service:8321']
#    use_gpu: bool [Default: False]
#    vector_store_name: str [Default: 'asr-vector-store']
components:
  comp-condition-2:
    dag:
      tasks:
        register-vector-store-and-files:
          cachingOptions: {}
          componentRef:
            name: comp-register-vector-store-and-files
          inputs:
            parameters:
              audio_filenames:
                componentInputParameter: pipelinechannel--audio_filenames
              base_url:
                componentInputParameter: pipelinechannel--base_url
              chunk_overlap_tokens:
                componentInputParameter: pipelinechannel--chunk_overlap_tokens
              embedding_model_id:
                componentInputParameter: pipelinechannel--embedding_model_id
              max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              service_url:
                componentInputParameter: pipelinechannel--service_url
              vector_store_name:
                componentInputParameter: pipelinechannel--vector_store_name
          taskInfo:
            name: register-vector-store-and-files
    inputDefinitions:
      parameters:
        pipelinechannel--audio_filenames:
          parameterType: STRING
        pipelinechannel--base_url:
          parameterType: STRING
        pipelinechannel--chunk_overlap_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--embedding_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_store_name:
          parameterType: STRING
  comp-condition-3:
    dag:
      tasks:
        register-vector-store-and-files-2:
          cachingOptions: {}
          componentRef:
            name: comp-register-vector-store-and-files-2
          inputs:
            parameters:
              audio_filenames:
                componentInputParameter: pipelinechannel--audio_filenames
              base_url:
                componentInputParameter: pipelinechannel--base_url
              chunk_overlap_tokens:
                componentInputParameter: pipelinechannel--chunk_overlap_tokens
              embedding_model_id:
                componentInputParameter: pipelinechannel--embedding_model_id
              max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              service_url:
                componentInputParameter: pipelinechannel--service_url
              vector_store_name:
                componentInputParameter: pipelinechannel--vector_store_name
          taskInfo:
            name: register-vector-store-and-files-2
    inputDefinitions:
      parameters:
        pipelinechannel--audio_filenames:
          parameterType: STRING
        pipelinechannel--base_url:
          parameterType: STRING
        pipelinechannel--chunk_overlap_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--embedding_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_store_name:
          parameterType: STRING
  comp-condition-branches-1:
    dag:
      tasks:
        condition-2:
          componentRef:
            name: comp-condition-2
          inputs:
            parameters:
              pipelinechannel--audio_filenames:
                componentInputParameter: pipelinechannel--audio_filenames
              pipelinechannel--base_url:
                componentInputParameter: pipelinechannel--base_url
              pipelinechannel--chunk_overlap_tokens:
                componentInputParameter: pipelinechannel--chunk_overlap_tokens
              pipelinechannel--embedding_model_id:
                componentInputParameter: pipelinechannel--embedding_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_store_name:
                componentInputParameter: pipelinechannel--vector_store_name
          taskInfo:
            name: condition-2
          triggerPolicy:
            condition: inputs.parameter_values['pipelinechannel--use_gpu'] == true
        condition-3:
          componentRef:
            name: comp-condition-3
          inputs:
            parameters:
              pipelinechannel--audio_filenames:
                componentInputParameter: pipelinechannel--audio_filenames
              pipelinechannel--base_url:
                componentInputParameter: pipelinechannel--base_url
              pipelinechannel--chunk_overlap_tokens:
                componentInputParameter: pipelinechannel--chunk_overlap_tokens
              pipelinechannel--embedding_model_id:
                componentInputParameter: pipelinechannel--embedding_model_id
              pipelinechannel--max_tokens:
                componentInputParameter: pipelinechannel--max_tokens
              pipelinechannel--service_url:
                componentInputParameter: pipelinechannel--service_url
              pipelinechannel--use_gpu:
                componentInputParameter: pipelinechannel--use_gpu
              pipelinechannel--vector_store_name:
                componentInputParameter: pipelinechannel--vector_store_name
          taskInfo:
            name: condition-3
          triggerPolicy:
            condition: '!(inputs.parameter_values[''pipelinechannel--use_gpu''] ==
              true)'
    inputDefinitions:
      parameters:
        pipelinechannel--audio_filenames:
          parameterType: STRING
        pipelinechannel--base_url:
          parameterType: STRING
        pipelinechannel--chunk_overlap_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--embedding_model_id:
          parameterType: STRING
        pipelinechannel--max_tokens:
          parameterType: NUMBER_INTEGER
        pipelinechannel--service_url:
          parameterType: STRING
        pipelinechannel--use_gpu:
          parameterType: BOOLEAN
        pipelinechannel--vector_store_name:
          parameterType: STRING
  comp-register-vector-store-and-files:
    executorLabel: exec-register-vector-store-and-files
    inputDefinitions:
      parameters:
        audio_filenames:
          parameterType: STRING
        base_url:
          parameterType: STRING
        chunk_overlap_tokens:
          parameterType: NUMBER_INTEGER
        embedding_model_id:
          parameterType: STRING
        max_tokens:
          parameterType: NUMBER_INTEGER
        service_url:
          parameterType: STRING
        vector_store_name:
          parameterType: STRING
  comp-register-vector-store-and-files-2:
    executorLabel: exec-register-vector-store-and-files-2
    inputDefinitions:
      parameters:
        audio_filenames:
          parameterType: STRING
        base_url:
          parameterType: STRING
        chunk_overlap_tokens:
          parameterType: NUMBER_INTEGER
        embedding_model_id:
          parameterType: STRING
        max_tokens:
          parameterType: NUMBER_INTEGER
        service_url:
          parameterType: STRING
        vector_store_name:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-register-vector-store-and-files:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - register_vector_store_and_files
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'llama-stack-client==0.3.0'\
          \ 'fire' 'requests' 'openai-whisper'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef register_vector_store_and_files(\n    service_url: str,\n   \
          \ vector_store_name: str,\n    embedding_model_id: str,\n    max_tokens:\
          \ int,\n    chunk_overlap_tokens: int,\n    base_url: str,\n    audio_filenames:\
          \ str,\n):\n    import io\n    import os\n    import tempfile\n    import\
          \ requests\n    import whisper\n    from llama_stack_client import LlamaStackClient\n\
          \n    def download_and_install_ffmpeg():\n        import os\n        import\
          \ subprocess\n\n        try:\n            print(\"Downloading static ffmpeg\
          \ binary...\")\n\n            import urllib.request\n            import\
          \ stat\n            import pathlib\n            import shutil\n\n      \
          \      # Create temp directory\n            temp_dir = pathlib.Path(\"/tmp/ffmpeg_install\"\
          )\n            temp_dir.mkdir(exist_ok=True)\n\n            # Download static\
          \ ffmpeg binary\n            ffmpeg_url = \"https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz\"\
          \n            ffmpeg_archive = temp_dir / \"ffmpeg-static.tar.xz\"\n\n \
          \           print(f\"Downloading ffmpeg from {ffmpeg_url}\")\n         \
          \   urllib.request.urlretrieve(ffmpeg_url, ffmpeg_archive)\n\n         \
          \   print(\"Extracting ffmpeg archive...\")\n            subprocess.run(\n\
          \                [\"tar\", \"-xf\", str(ffmpeg_archive), \"-C\", str(temp_dir)],\n\
          \                check=True,\n                capture_output=True,\n   \
          \         )\n\n            ffmpeg_path_candidates = list(temp_dir.rglob(\"\
          ffmpeg\"))\n            if not ffmpeg_path_candidates:\n               \
          \ raise FileNotFoundError(\n                    \"Could not find 'ffmpeg'\
          \ executable in the extracted archive.\"\n                )\n\n        \
          \    ffmpeg_path = ffmpeg_path_candidates[0]\n\n            # Make executable\n\
          \            ffmpeg_path.chmod(\n                ffmpeg_path.stat().st_mode\
          \ | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH\n            )\n\n      \
          \      # Use a writable bin directory and add it to the PATH\n         \
          \   bin_dir = pathlib.Path(\"/tmp/bin\")\n            bin_dir.mkdir(exist_ok=True)\n\
          \            target_path = bin_dir / \"ffmpeg\"\n\n            # Move the\
          \ file to the target path\n            ffmpeg_path.rename(target_path)\n\
          \n            # Add to PATH environment variable\n            os.environ[\"\
          PATH\"] = f\"{str(bin_dir)}:{os.environ.get('PATH', '')}\"\n\n         \
          \   # Verify installation\n            subprocess.run(\n               \
          \ [str(target_path), \"-version\"], capture_output=True, check=True\n  \
          \          )\n            print(f\"Static ffmpeg binary installed to {target_path}\
          \ and added to PATH\")\n\n            # Clean up extraction directory\n\
          \            shutil.rmtree(temp_dir, ignore_errors=True)\n\n        except\
          \ Exception as e:\n            print(f\"Failed to install ffmpeg: {e}\"\
          )\n            raise RuntimeError(\n                \"ffmpeg installation\
          \ failed. Audio processing requires ffmpeg.\"\n            ) from e\n\n\
          \    # Install ffmpeg for audio processing before main logic\n    download_and_install_ffmpeg()\n\
          \n    # Load Whisper model\n    print(\"Loading Whisper model...\")\n  \
          \  whisper_model = whisper.load_model(\"base\")\n\n    client = LlamaStackClient(base_url=service_url)\n\
          \n    # Transcribe audio files and upload transcripts\n    file_ids = []\n\
          \    for filename in audio_filenames.split(\",\"):\n        source = f\"\
          {base_url}/{filename.strip()}\"\n        print(f\"Downloading audio file:\
          \ {source}\")\n\n        try:\n            # Download the audio file\n \
          \           response = requests.get(source)\n            response.raise_for_status()\n\
          \n            # Save to temporary file\n            with tempfile.NamedTemporaryFile(suffix=\"\
          .wav\", delete=False) as tmp_audio:\n                tmp_audio.write(response.content)\n\
          \                tmp_audio_path = tmp_audio.name\n\n            try:\n \
          \               # Transcribe with Whisper\n                print(f\"Transcribing\
          \ {filename.strip()}...\")\n                result = whisper_model.transcribe(tmp_audio_path)\n\
          \                transcript_text = result[\"text\"]\n                print(f\"\
          Transcription complete: {len(transcript_text)} characters\")\n\n       \
          \         # Upload transcript as text file\n                file_basename\
          \ = filename.strip().rsplit(\".\", 1)[0] + \".txt\"\n                transcript_content\
          \ = io.BytesIO(transcript_text.encode(\"utf-8\"))\n\n                file\
          \ = client.files.create(\n                    file=(file_basename, transcript_content,\
          \ \"text/plain\"),\n                    purpose=\"assistants\",\n      \
          \          )\n                file_ids.append(file.id)\n               \
          \ print(\n                    f\"Successfully uploaded transcript {file_basename}\
          \ (file_id: {file.id})\"\n                )\n\n            finally:\n  \
          \              # Clean up temp file\n                os.unlink(tmp_audio_path)\n\
          \n        except Exception as e:\n            print(f\"ERROR: Failed to\
          \ process {filename.strip()}: {str(e)}\")\n            raise\n\n    print(\n\
          \        f\"Successfully processed and uploaded {len(file_ids)} transcripts:\
          \ {file_ids}\"\n    )\n\n    models = client.models.list()\n    matching_model\
          \ = next(\n        (m for m in models if m.provider_resource_id == embedding_model_id),\
          \ None\n    )\n\n    if not matching_model:\n        raise ValueError(\n\
          \            f\"Model with ID '{embedding_model_id}' not found on LlamaStack\
          \ server.\"\n        )\n\n    if matching_model.api_model_type != \"embedding\"\
          :\n        raise ValueError(f\"Model '{embedding_model_id}' is not an embedding\
          \ model\")\n\n    embedding_dimension = matching_model.metadata[\"embedding_dimension\"\
          ]\n\n    # Warm up the embedding model\n    client.embeddings.create(\n\
          \        model=embedding_model_id,\n        input=\"warmup\",\n    )\n\n\
          \    # Create empty vector store first, before inserting files.\n    # Purpose:\
          \ Depending on the size and number of files, attempting to create the vector\
          \ store\n    # and add files in a single step may lead to timeouts.\n  \
          \  try:\n        vector_store = client.vector_stores.create(\n         \
          \   name=vector_store_name,\n            file_ids=[],\n            chunking_strategy={\n\
          \                \"type\": \"static\",\n                \"static\": {\n\
          \                    \"max_chunk_size_tokens\": max_tokens,\n          \
          \          \"chunk_overlap_tokens\": chunk_overlap_tokens,\n           \
          \     },\n            },\n            extra_body={\n                \"embedding_model\"\
          : embedding_model_id,\n                \"embedding_dimension\": embedding_dimension,\n\
          \                \"provider_id\": \"milvus\",\n            },\n        )\n\
          \        print(\n            f\"Successfully created vector store '{vector_store_name}'\
          \ with ID: {vector_store.id}\"\n        )\n    except Exception as e:\n\
          \        print(f\"ERROR: Failed to create vector store '{vector_store_name}':\
          \ {str(e)}\")\n        raise\n\n    # Add files to vector store\n    try:\n\
          \        for file_id in file_ids:\n            print(f\"Adding file_id '{file_id}'\
          \ to vector store '{vector_store_name}'\")\n            client.vector_stores.files.create(\n\
          \                vector_store_id=vector_store.id,\n                file_id=file_id,\n\
          \            )\n        vector_store = client.vector_stores.retrieve(vector_store.id)\n\
          \        print(f\"Vector store details: {vector_store}\")\n    except Exception\
          \ as e:\n        print(f\"WARNING: Some files failed to be added to vector\
          \ store: {str(e)}\")\n\n"
        image: quay.io/modh/odh-pipeline-runtime-pytorch-cuda-py312-ubi9@sha256:72ff2381e5cb24d6f549534cb74309ed30e92c1ca80214669adb78ad30c5ae12
        resources:
          accelerator:
            count: '1'
            resourceCount: '1'
            resourceType: nvidia.com/gpu
            type: nvidia.com/gpu
          cpuLimit: 4.0
          cpuRequest: 0.5
          memoryLimit: 6.442450944
          memoryRequest: 2.147483648
          resourceCpuLimit: '4'
          resourceCpuRequest: 500m
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 2Gi
    exec-register-vector-store-and-files-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - register_vector_store_and_files
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'llama-stack-client==0.3.0'\
          \ 'fire' 'requests' 'openai-whisper'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.14.6' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef register_vector_store_and_files(\n    service_url: str,\n   \
          \ vector_store_name: str,\n    embedding_model_id: str,\n    max_tokens:\
          \ int,\n    chunk_overlap_tokens: int,\n    base_url: str,\n    audio_filenames:\
          \ str,\n):\n    import io\n    import os\n    import tempfile\n    import\
          \ requests\n    import whisper\n    from llama_stack_client import LlamaStackClient\n\
          \n    def download_and_install_ffmpeg():\n        import os\n        import\
          \ subprocess\n\n        try:\n            print(\"Downloading static ffmpeg\
          \ binary...\")\n\n            import urllib.request\n            import\
          \ stat\n            import pathlib\n            import shutil\n\n      \
          \      # Create temp directory\n            temp_dir = pathlib.Path(\"/tmp/ffmpeg_install\"\
          )\n            temp_dir.mkdir(exist_ok=True)\n\n            # Download static\
          \ ffmpeg binary\n            ffmpeg_url = \"https://github.com/BtbN/FFmpeg-Builds/releases/download/latest/ffmpeg-master-latest-linux64-gpl.tar.xz\"\
          \n            ffmpeg_archive = temp_dir / \"ffmpeg-static.tar.xz\"\n\n \
          \           print(f\"Downloading ffmpeg from {ffmpeg_url}\")\n         \
          \   urllib.request.urlretrieve(ffmpeg_url, ffmpeg_archive)\n\n         \
          \   print(\"Extracting ffmpeg archive...\")\n            subprocess.run(\n\
          \                [\"tar\", \"-xf\", str(ffmpeg_archive), \"-C\", str(temp_dir)],\n\
          \                check=True,\n                capture_output=True,\n   \
          \         )\n\n            ffmpeg_path_candidates = list(temp_dir.rglob(\"\
          ffmpeg\"))\n            if not ffmpeg_path_candidates:\n               \
          \ raise FileNotFoundError(\n                    \"Could not find 'ffmpeg'\
          \ executable in the extracted archive.\"\n                )\n\n        \
          \    ffmpeg_path = ffmpeg_path_candidates[0]\n\n            # Make executable\n\
          \            ffmpeg_path.chmod(\n                ffmpeg_path.stat().st_mode\
          \ | stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH\n            )\n\n      \
          \      # Use a writable bin directory and add it to the PATH\n         \
          \   bin_dir = pathlib.Path(\"/tmp/bin\")\n            bin_dir.mkdir(exist_ok=True)\n\
          \            target_path = bin_dir / \"ffmpeg\"\n\n            # Move the\
          \ file to the target path\n            ffmpeg_path.rename(target_path)\n\
          \n            # Add to PATH environment variable\n            os.environ[\"\
          PATH\"] = f\"{str(bin_dir)}:{os.environ.get('PATH', '')}\"\n\n         \
          \   # Verify installation\n            subprocess.run(\n               \
          \ [str(target_path), \"-version\"], capture_output=True, check=True\n  \
          \          )\n            print(f\"Static ffmpeg binary installed to {target_path}\
          \ and added to PATH\")\n\n            # Clean up extraction directory\n\
          \            shutil.rmtree(temp_dir, ignore_errors=True)\n\n        except\
          \ Exception as e:\n            print(f\"Failed to install ffmpeg: {e}\"\
          )\n            raise RuntimeError(\n                \"ffmpeg installation\
          \ failed. Audio processing requires ffmpeg.\"\n            ) from e\n\n\
          \    # Install ffmpeg for audio processing before main logic\n    download_and_install_ffmpeg()\n\
          \n    # Load Whisper model\n    print(\"Loading Whisper model...\")\n  \
          \  whisper_model = whisper.load_model(\"base\")\n\n    client = LlamaStackClient(base_url=service_url)\n\
          \n    # Transcribe audio files and upload transcripts\n    file_ids = []\n\
          \    for filename in audio_filenames.split(\",\"):\n        source = f\"\
          {base_url}/{filename.strip()}\"\n        print(f\"Downloading audio file:\
          \ {source}\")\n\n        try:\n            # Download the audio file\n \
          \           response = requests.get(source)\n            response.raise_for_status()\n\
          \n            # Save to temporary file\n            with tempfile.NamedTemporaryFile(suffix=\"\
          .wav\", delete=False) as tmp_audio:\n                tmp_audio.write(response.content)\n\
          \                tmp_audio_path = tmp_audio.name\n\n            try:\n \
          \               # Transcribe with Whisper\n                print(f\"Transcribing\
          \ {filename.strip()}...\")\n                result = whisper_model.transcribe(tmp_audio_path)\n\
          \                transcript_text = result[\"text\"]\n                print(f\"\
          Transcription complete: {len(transcript_text)} characters\")\n\n       \
          \         # Upload transcript as text file\n                file_basename\
          \ = filename.strip().rsplit(\".\", 1)[0] + \".txt\"\n                transcript_content\
          \ = io.BytesIO(transcript_text.encode(\"utf-8\"))\n\n                file\
          \ = client.files.create(\n                    file=(file_basename, transcript_content,\
          \ \"text/plain\"),\n                    purpose=\"assistants\",\n      \
          \          )\n                file_ids.append(file.id)\n               \
          \ print(\n                    f\"Successfully uploaded transcript {file_basename}\
          \ (file_id: {file.id})\"\n                )\n\n            finally:\n  \
          \              # Clean up temp file\n                os.unlink(tmp_audio_path)\n\
          \n        except Exception as e:\n            print(f\"ERROR: Failed to\
          \ process {filename.strip()}: {str(e)}\")\n            raise\n\n    print(\n\
          \        f\"Successfully processed and uploaded {len(file_ids)} transcripts:\
          \ {file_ids}\"\n    )\n\n    models = client.models.list()\n    matching_model\
          \ = next(\n        (m for m in models if m.provider_resource_id == embedding_model_id),\
          \ None\n    )\n\n    if not matching_model:\n        raise ValueError(\n\
          \            f\"Model with ID '{embedding_model_id}' not found on LlamaStack\
          \ server.\"\n        )\n\n    if matching_model.api_model_type != \"embedding\"\
          :\n        raise ValueError(f\"Model '{embedding_model_id}' is not an embedding\
          \ model\")\n\n    embedding_dimension = matching_model.metadata[\"embedding_dimension\"\
          ]\n\n    # Warm up the embedding model\n    client.embeddings.create(\n\
          \        model=embedding_model_id,\n        input=\"warmup\",\n    )\n\n\
          \    # Create empty vector store first, before inserting files.\n    # Purpose:\
          \ Depending on the size and number of files, attempting to create the vector\
          \ store\n    # and add files in a single step may lead to timeouts.\n  \
          \  try:\n        vector_store = client.vector_stores.create(\n         \
          \   name=vector_store_name,\n            file_ids=[],\n            chunking_strategy={\n\
          \                \"type\": \"static\",\n                \"static\": {\n\
          \                    \"max_chunk_size_tokens\": max_tokens,\n          \
          \          \"chunk_overlap_tokens\": chunk_overlap_tokens,\n           \
          \     },\n            },\n            extra_body={\n                \"embedding_model\"\
          : embedding_model_id,\n                \"embedding_dimension\": embedding_dimension,\n\
          \                \"provider_id\": \"milvus\",\n            },\n        )\n\
          \        print(\n            f\"Successfully created vector store '{vector_store_name}'\
          \ with ID: {vector_store.id}\"\n        )\n    except Exception as e:\n\
          \        print(f\"ERROR: Failed to create vector store '{vector_store_name}':\
          \ {str(e)}\")\n        raise\n\n    # Add files to vector store\n    try:\n\
          \        for file_id in file_ids:\n            print(f\"Adding file_id '{file_id}'\
          \ to vector store '{vector_store_name}'\")\n            client.vector_stores.files.create(\n\
          \                vector_store_id=vector_store.id,\n                file_id=file_id,\n\
          \            )\n        vector_store = client.vector_stores.retrieve(vector_store.id)\n\
          \        print(f\"Vector store details: {vector_store}\")\n    except Exception\
          \ as e:\n        print(f\"WARNING: Some files failed to be added to vector\
          \ store: {str(e)}\")\n\n"
        image: quay.io/modh/odh-pipeline-runtime-pytorch-cuda-py312-ubi9@sha256:72ff2381e5cb24d6f549534cb74309ed30e92c1ca80214669adb78ad30c5ae12
        resources:
          cpuLimit: 4.0
          cpuRequest: 0.5
          memoryLimit: 6.442450944
          memoryRequest: 2.147483648
          resourceCpuLimit: '4'
          resourceCpuRequest: 500m
          resourceMemoryLimit: 6Gi
          resourceMemoryRequest: 2Gi
pipelineInfo:
  description: Converts audio recordings to text using WHISPER ASR and generates embeddings
  name: vector-store-files-pipeline
root:
  dag:
    tasks:
      condition-branches-1:
        componentRef:
          name: comp-condition-branches-1
        inputs:
          parameters:
            pipelinechannel--audio_filenames:
              componentInputParameter: audio_filenames
            pipelinechannel--base_url:
              componentInputParameter: base_url
            pipelinechannel--chunk_overlap_tokens:
              componentInputParameter: chunk_overlap_tokens
            pipelinechannel--embedding_model_id:
              componentInputParameter: embedding_model_id
            pipelinechannel--max_tokens:
              componentInputParameter: max_tokens
            pipelinechannel--service_url:
              componentInputParameter: service_url
            pipelinechannel--use_gpu:
              componentInputParameter: use_gpu
            pipelinechannel--vector_store_name:
              componentInputParameter: vector_store_name
        taskInfo:
          name: condition-branches-1
  inputDefinitions:
    parameters:
      audio_filenames:
        defaultValue: RAG_use_cases.wav, RAG_customers.wav, RAG_benefits.m4a, RAG_vs_Regular_LLM_Output.m4a
        description: Comma-separated list of audio filenames to download and convert
        isOptional: true
        parameterType: STRING
      base_url:
        defaultValue: https://raw.githubusercontent.com/opendatahub-io/rag/main/demos/testing-data/audio-speech
        description: Base URL to fetch audio files
        isOptional: true
        parameterType: STRING
      chunk_overlap_tokens:
        defaultValue: 64.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      embedding_model_id:
        defaultValue: ibm-granite/granite-embedding-125m-english
        description: Model ID for embedding generation
        isOptional: true
        parameterType: STRING
      max_tokens:
        defaultValue: 512.0
        description: Maximum number of tokens per chunk
        isOptional: true
        parameterType: NUMBER_INTEGER
      service_url:
        defaultValue: http://lsd-milvus-service:8321
        description: URL of the LlamaStack service
        isOptional: true
        parameterType: STRING
      use_gpu:
        defaultValue: false
        description: boolean to enable/disable gpu
        isOptional: true
        parameterType: BOOLEAN
      vector_store_name:
        defaultValue: asr-vector-store
        description: Name of the vector store to store embeddings
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.6
---
platforms:
  kubernetes:
    deploymentSpec:
      executors:
        exec-register-vector-store-and-files:
          nodeSelector:
            nodeSelectorJson:
              runtimeValue:
                constant: {}
          tolerations:
          - effect: NoSchedule
            key: nvidia.com/gpu
            operator: Exists
