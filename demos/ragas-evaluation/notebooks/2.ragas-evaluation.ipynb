{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fad8144",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation with Llama Stack - Demo\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use the RAGAS (Retrieval-Augmented Generation Assessment) out-of-tree provider with Llama Stack to evaluate RAG systems.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete **[`1.dataset_generation.ipynb`](1.dataset_generation.ipynb)** first to generate the evaluation dataset\n",
    "- Ensure the `rag_evaluation_dataset.jsonl` file exists in the current directory\n",
    "\n",
    "This notebook will:\n",
    "1. Load the evaluation dataset generated in the previous notebook\n",
    "2. Register the dataset with Llama Stack\n",
    "3. Configure RAGAS evaluation metrics\n",
    "4. Run evaluation using the RAGAS provider (remote mode via Kubeflow Pipelines)\n",
    "5. Display and analyze the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17973f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea18f6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from rich.pretty import pprint\n",
    "\n",
    "from llama_stack_provider_ragas.constants import PROVIDER_ID_REMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46619740",
   "metadata": {},
   "source": [
    "## Llama Stack Client Setup\n",
    "\n",
    "This step verifies that:\n",
    "- **Llama Stack is reachable** - The client can successfully connect to the Llama Stack service\n",
    "- **Required models are available** - Both an inference model (`model_type='llm'`) and an embedding model (`model_type='embedding'`) are registered and accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1945e15",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "client = LlamaStackClient(base_url=\"http://lsd-ragas-example-service:8321\")\n",
    "available_models = client.models.list()\n",
    "assert any(model.model_type == \"llm\" for model in available_models)\n",
    "assert any(model.model_type == \"embedding\" for model in available_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d2031",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "Load the RAG evaluation dataset generated from the previous notebook (`1.dataset_generation.ipynb`).\n",
    "\n",
    "The dataset should be in JSONL format with the following structure:\n",
    "- `question`: The question to evaluate\n",
    "- `answer`: The generated answer\n",
    "- `contexts`: List of context strings used for answer generation\n",
    "- `ground_truth`: The ground truth answer or reference\n",
    "\n",
    "**Note:** If you haven't run the first notebook yet, please complete it first to generate the `rag_evaluation_dataset.jsonl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0c2f3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "jsonl_path = \"rag_evaluation_dataset.jsonl\"\n",
    "evaluation_data = []\n",
    "\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():  # Skip empty lines\n",
    "            data = json.loads(line)\n",
    "            evaluation_data.append({\n",
    "                \"user_input\": data[\"question\"],\n",
    "                \"response\": data[\"answer\"],\n",
    "                \"retrieved_contexts\": data[\"contexts\"],\n",
    "                \"reference\": data[\"ground_truth\"],\n",
    "            })\n",
    "\n",
    "print(f\"Loaded {len(evaluation_data)} evaluation examples from {jsonl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d2dcd2",
   "metadata": {},
   "source": [
    "## Dataset Registration\n",
    "\n",
    "Register the dataset with Llama Stack's Datasets API using the direct rows approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485538a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# De-register the dataset if it already exists\n",
    "dataset_id = \"ragas_demo_dataset\"\n",
    "try:\n",
    "    client.datasets.unregister(dataset_id)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "dataset_response = client.datasets.register(\n",
    "    dataset_id=dataset_id,\n",
    "    purpose=\"eval/question-answer\",\n",
    "    source={\"type\": \"rows\", \"rows\": evaluation_data},\n",
    "    metadata={\n",
    "        \"provider_id\": \"localfs\",\n",
    "        \"description\": \"Sample RAG evaluation dataset for Ragas demo\",\n",
    "        \"size\": len(evaluation_data),\n",
    "        \"format\": \"ragas\",\n",
    "        \"created_at\": datetime.now().isoformat(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435db6fc",
   "metadata": {},
   "source": [
    "## Benchmark Registration\n",
    "\n",
    "A benchmark defines which RAGAS metrics to use for evaluation. Register one or more benchmarks with different providers or metric configurations.\n",
    "\n",
    "### Available RAGAS Metrics\n",
    "\n",
    "- **`answer_relevancy`**: Measures how relevant the generated answer is to the question (0-1, higher is better)\n",
    "- **`context_precision`**: Measures how precise the retrieved contexts are for answering the question\n",
    "- **`faithfulness`**: Measures how faithful the answer is to the retrieved contexts (checks for hallucinations)\n",
    "- **`context_recall`**: Measures how much of the ground truth information is covered by the retrieved contexts\n",
    "- **`answer_correctness`**: Measures how correct the answer is compared to the ground truth (combines semantic similarity and factual accuracy)\n",
    "\n",
    "**Note:** You can enable multiple metrics by uncommenting them in the `scoring_functions` list below. More metrics provide more comprehensive evaluation but may increase computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5102a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "# Select which benchmark to use for evaluation\nbenchmark_id = \"ragas_demo_benchmark__remote\"\n\nbenchmarks_to_register = [\n    (benchmark_id, PROVIDER_ID_REMOTE),\n]\n\n# Configure which RAGAS metrics to evaluate\n# Uncomment additional metrics if needed\nragas_metrics = [\n    \"answer_relevancy\",        # Measures answer relevance to the question\n    # \"context_precision\",     # Measures precision of retrieved contexts\n    # \"faithfulness\",          # Measures faithfulness to contexts\n    # \"context_recall\",        # Measures coverage of ground truth by contexts\n    # \"answer_correctness\",    # TODO: Currently throws error 500 - do not enable\n]\n\n# Register benchmarks\nfor benchmark_id_to_register, provider_id in benchmarks_to_register:\n    # Unregister existing benchmark if it exists\n    try:\n        client.benchmarks.unregister(benchmark_id_to_register)\n        print(f\"Unregistered existing benchmark: {benchmark_id_to_register}\")\n    except Exception:\n        pass  # Benchmark doesn't exist yet, which is fine\n    \n    # Register the benchmark\n    benchmark_response = client.benchmarks.register(\n        benchmark_id=benchmark_id_to_register,\n        dataset_id=dataset_id,\n        scoring_functions=ragas_metrics,\n        provider_id=provider_id,\n    )\n\nbenchmarks = client.benchmarks.list()\npprint(benchmarks)"
  },
  {
   "cell_type": "markdown",
   "id": "2038b8b4",
   "metadata": {},
   "source": [
    "## Evaluation Execution\n",
    "\n",
    "Run the evaluation using our Ragas out-of-tree provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c33086",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "remote_job = client.alpha.eval.run_eval(\n",
    "    benchmark_id=benchmark_id,\n",
    "    benchmark_config={\n",
    "        \"eval_candidate\": {\n",
    "            \"type\": \"model\",\n",
    "            \"model\": \"vllm-inference/qwen3-14b-awq\",\n",
    "            \"sampling_params\": {\"temperature\": 0.1, \"max_tokens\": 100},\n",
    "        },\n",
    "        \"scoring_params\": {},\n",
    "        # \"num_examples\": 1,\n",
    "    },\n",
    ")\n",
    "pprint(remote_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49ab649",
   "metadata": {},
   "source": [
    "## Results Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533f42f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Wait a bit for the job to complete\n",
    "import time\n",
    "remote_job_status = client.alpha.eval.jobs.status(\n",
    "    benchmark_id=benchmark_id, job_id=remote_job.job_id\n",
    ")\n",
    "while remote_job_status.status not in (\"failed\", \"completed\"):\n",
    "    print(f\"Status: {remote_job_status.status}\")\n",
    "    time.sleep(10)\n",
    "    remote_job_status = client.alpha.eval.jobs.status(\n",
    "        benchmark_id=benchmark_id, job_id=remote_job.job_id\n",
    "    )\n",
    "    \n",
    "pprint(\n",
    "    client.alpha.eval.jobs.status(\n",
    "        benchmark_id=benchmark_id, job_id=remote_job.job_id\n",
    "    )\n",
    ")\n",
    "remote_results = client.alpha.eval.jobs.retrieve(\n",
    "    benchmark_id=benchmark_id, job_id=remote_job.job_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ca28e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Print all metrics from remote results in a single table\n",
    "df = pd.concat(\n",
    "    {metric: pd.Series([r[\"score\"] for r in result.score_rows])\n",
    "     for metric, result in remote_results.scores.items()},\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb14bf35",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ðŸŽ‰ You have successfully completed the RAGAS evaluation demo!\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "1. âœ… Loaded the evaluation dataset generated from the SDG Hub RAG Flow\n",
    "2. âœ… Registered the dataset with Llama Stack\n",
    "3. âœ… Configured RAGAS evaluation metrics\n",
    "4. âœ… Run evaluation using the RAGAS remote provider via Kubeflow Pipelines\n",
    "5. âœ… Analyzed and visualized the evaluation results\n",
    "\n",
    "### Understanding the Results\n",
    "\n",
    "The RAGAS metrics provide insights into your RAG system's performance:\n",
    "\n",
    "- **Answer Relevancy**: Measures how relevant the answer is to the question\n",
    "- **Faithfulness**: Measures how faithful the answer is to the retrieved contexts\n",
    "- **Context Precision**: Measures how precise the retrieved contexts are\n",
    "- **Context Recall**: Measures how much of the ground truth is covered by contexts\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
