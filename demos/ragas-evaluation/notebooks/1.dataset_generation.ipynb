{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07483ff3",
   "metadata": {},
   "source": [
    "# RAG Evaluation Dataset Generation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook is **Part 1 of 2** in the RAGAS Evaluation Demo. It demonstrates how to generate RAG (Retrieval-Augmented Generation) evaluation datasets using the SDG Hub framework. It creates question-answer pairs with ground truth context that can be used to evaluate RAG systems.\n",
    "\n",
    "**Next:** After completing this notebook, proceed to **[`2.ragas-evaluation.ipynb`](2.ragas-evaluation.ipynb)** to evaluate your RAG system using RAGAS metrics.\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "1. **Construct Input Dataset**: Show how to prepare documents with outlines for the RAG evaluation flow\n",
    "2. **Generate RAG Evaluation Dataset**: Run the RAG Evaluation flow to create question-answer pairs with:\n",
    "   - Topic extraction from documents\n",
    "   - Conceptual question generation\n",
    "   - Question evolution for better quality\n",
    "   - Answer generation with grounding\n",
    "   - Groundedness scoring and filtering\n",
    "   - Ground truth context extraction\n",
    "3. **Post-process for Evaluation**: Convert the output to evaluation-ready formats (e.g., for RAGAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac413490",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a98fe5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "from sdg_hub import Flow, FlowRegistry\n",
    "\n",
    "# Required to run the flow with async mode\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b74db7",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Input Dataset\n",
    "\n",
    "The RAG Evaluation flow requires:\n",
    "- **document**: The full text content of the document\n",
    "- **document_outline**: A concise title or summary that represents the document\n",
    "\n",
    "You can prepare this from various sources:\n",
    "- PDF documents (extract text and create outlines)\n",
    "- Text files\n",
    "- Existing datasets\n",
    "- Web content\n",
    "\n",
    "Below are example functions to help construct the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf7b48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataset_from_text(text: str, document_outline: str, chunk_size: int = 3000, overlap: int = 500):\n",
    "    \"\"\"\n",
    "    Prepare dataset from a single text document by chunking it.\n",
    "    \n",
    "    Args:\n",
    "        text: Full document text\n",
    "        document_outline: Title or summary of the document\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Overlap between chunks to maintain context (must be < chunk_size)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset with document and document_outline columns\n",
    "    \"\"\"\n",
    "    # Validate parameters\n",
    "    if overlap >= chunk_size:\n",
    "        raise ValueError(f\"overlap ({overlap}) must be less than chunk_size ({chunk_size})\")\n",
    "    \n",
    "    if chunk_size <= 0:\n",
    "        raise ValueError(f\"chunk_size must be positive, got {chunk_size}\")\n",
    "    \n",
    "    # Simple chunking by character count with overlap\n",
    "    chunks = []\n",
    "    step_size = chunk_size - overlap\n",
    "    \n",
    "    for i in range(0, len(text), step_size):\n",
    "        chunk = text[i:i + chunk_size]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict({\n",
    "        \"document\": chunks,\n",
    "        \"document_outline\": [document_outline] * len(chunks)\n",
    "    })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks from document\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prepare_dataset_from_pdf(pdf_path: str, document_outline: str, max_pages: int = None):\n",
    "    \"\"\"\n",
    "    Prepare dataset from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        document_outline: Title or summary of the document\n",
    "        max_pages: Maximum number of pages to process (None for all)\n",
    "        \n",
    "    Returns:\n",
    "        Dataset with document and document_outline columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "    except ImportError:\n",
    "        raise ImportError(\"PyPDF2 is required. Install with: pip install PyPDF2\")\n",
    "    \n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    pages_to_read = reader.pages[:max_pages] if max_pages else reader.pages\n",
    "    for page in pages_to_read:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "    \n",
    "    return prepare_dataset_from_text(text, document_outline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83288af1",
   "metadata": {},
   "source": [
    "### Example: Create Dataset from IBM Annual Report\n",
    "\n",
    "Here's an example using the IBM 2024 Annual Report. It will extract text from the first 20 pages and create chunks for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d8c50b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pdf_path = \"ibm-annual-report-2024.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"PDF file not found: {pdf_path}\\n\"\n",
    "    )\n",
    "\n",
    "input_dataset = prepare_dataset_from_pdf(pdf_path, \"IBM 2024 Annual Report Summary\", max_pages=20)\n",
    "print(f\"\\nInput dataset columns: {input_dataset.column_names}\")\n",
    "print(f\"Number of samples: {len(input_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e93a196",
   "metadata": {},
   "source": [
    "## Step 2: Discover and Load the RAG Evaluation Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fc5b50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the RAG Evaluation flow\n",
    "flow_name = \"RAG Evaluation Dataset Flow\"\n",
    "flow_path = FlowRegistry.get_flow_path(flow_name)\n",
    "\n",
    "flow = Flow.from_yaml(flow_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a130d46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def set_model_config(flow_object):\n",
    "    \"\"\"Configure the model for the flow based on environment variables.\"\"\"\n",
    "    model = os.getenv(\"INFERENCE_MODEL\", \"vllm-inference/qwen3-14b-awq\")\n",
    "    api_base = os.getenv(\"URL\", \"http://lsd-ragas-example-service:8321/v1\")\n",
    "    api_key = os.getenv(\"API_KEY\", \"fake\")\n",
    "    \n",
    "    if model and not model.startswith(\"openai/\") and not model.startswith(\"ollama/\"):\n",
    "        model = \"openai/\" + model\n",
    "    \n",
    "    print(f\"Configuring model: {model}\")\n",
    "    \n",
    "    flow_object.set_model_config(\n",
    "        model=model,\n",
    "        api_base=api_base if api_base else None,\n",
    "        api_key=api_key if api_key else None,\n",
    "    )\n",
    "    \n",
    "    return flow_object\n",
    "\n",
    "# Configure the model\n",
    "flow = set_model_config(flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf67b07",
   "metadata": {},
   "source": [
    "## Step 4: Generate RAG Evaluation Dataset\n",
    "\n",
    "Run the flow to generate question-answer pairs with ground truth context. The flow will:\n",
    "1. Extract topics from documents\n",
    "2. Generate conceptual questions\n",
    "3. Evolve questions for better quality\n",
    "4. Generate answers with grounding\n",
    "5. Score groundedness and filter low-quality pairs\n",
    "6. Extract ground truth context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc86d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get runtime parameters\n",
    "max_concurrency = int(os.getenv(\"MAX_CONCURRENCY\", \"10\"))\n",
    "\n",
    "# Optional: Configure runtime parameters for specific blocks\n",
    "runtime_params = {}\n",
    "\n",
    "print(\"This may take several minutes depending on dataset size and model speed...\\n\")\n",
    "\n",
    "# Generate the dataset\n",
    "generated_data = flow.generate(\n",
    "    input_dataset, \n",
    "    runtime_params=runtime_params, \n",
    "    max_concurrency=max_concurrency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2939c1e",
   "metadata": {},
   "source": [
    "## Step 5: Post-process for Evaluation\n",
    "\n",
    "Convert the generated dataset to evaluation-ready formats. This prepares the data for use with evaluation frameworks like RAGAS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf729503",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "df = generated_data.to_pandas()\n",
    "\n",
    "def prepare_for_ragas_evaluation(generated_df: pd.DataFrame, output_file: str = None):\n",
    "    \"\"\"\n",
    "    Convert generated dataset to RAGAS evaluation format.\n",
    "    \n",
    "    RAGAS expects:\n",
    "    - question: The question\n",
    "    - answer: The generated answer\n",
    "    - contexts: List of context strings (usually one)\n",
    "    - ground_truth: The ground truth answer (can be same as answer or use ground_truth_context)\n",
    "    \n",
    "    Args:\n",
    "        generated_df: DataFrame from flow generation\n",
    "        output_file: Optional path to save JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries in RAGAS format\n",
    "    \"\"\"\n",
    "    ragas_data = []\n",
    "    \n",
    "    for _, row in generated_df.iterrows():\n",
    "        question = row.get('question', '')\n",
    "        answer = row.get('response', '')\n",
    "        context = row.get('document', row.get('context', ''))\n",
    "        ground_truth = row.get('ground_truth_context', answer)\n",
    "        \n",
    "        ragas_record = {\n",
    "            \"question\": str(question),\n",
    "            \"answer\": str(answer),\n",
    "            \"contexts\": [str(context)] if context else [\"\"],\n",
    "            \"ground_truth\": str(ground_truth)\n",
    "        }\n",
    "        \n",
    "        ragas_data.append(ragas_record)\n",
    "    \n",
    "    if output_file:\n",
    "        output_file = Path(output_file)\n",
    "        output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with output_file.open(\"w\") as f:\n",
    "            for record in ragas_data:\n",
    "                f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    return ragas_data\n",
    "\n",
    "ragas_data = prepare_for_ragas_evaluation(df, output_file=\"rag_evaluation_dataset.jsonl\")\n",
    "\n",
    "# Save the full generated dataset\n",
    "output_csv = \"rag_evaluation_full_results.csv\"\n",
    "generated_data.to_csv(output_csv, index=False)\n",
    "print(f\"Saved full results to {output_csv}\")\n",
    "\n",
    "print(f\"\\nâœ… Prepared {len(ragas_data)} records for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c4bb09",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "ðŸŽ‰ You have successfully:\n",
    "\n",
    "1. âœ… Prepared input dataset with documents and outlines\n",
    "2. âœ… Generated RAG evaluation dataset with question-answer pairs\n",
    "3. âœ… Post-processed data for evaluation frameworks\n",
    "\n",
    "### Generated Files\n",
    "\n",
    "- `rag_evaluation_dataset.jsonl` - RAGAS-ready evaluation dataset\n",
    "- `rag_evaluation_full_results.csv` - Full generated results with all metadata\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "**âž¡ï¸ Continue to the RAGAS Evaluation Notebook**\n",
    "\n",
    "Now that you have generated your evaluation dataset, proceed to **[`2.ragas-evaluation.ipynb`](2.ragas-evaluation.ipynb)** to:\n",
    "- Load the generated dataset\n",
    "- Run RAGAS evaluation metrics using the Llama Stack RAGAS provider\n",
    "- Analyze evaluation results and metrics\n",
    "\n",
    "The second notebook will use the `rag_evaluation_dataset.jsonl` file created in this notebook to perform RAG evaluation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
